## Machine Learning

### Definition
- Machine Learning is the science (and art) of programming computers so they can learn from data.
- Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.
- A computer program is said to learn from experience E with respect to some task T and some performance measure P, 
if its performance on T, as measured by P, improves with experience E.

### Machine Learning Type
  - Supervised(监督学习)
  - Unsupervised(非监督学习)
  - Semisupervised(半监督学习)
  - Reinforcement(强制学习)
  - Online(在线学习)
  - Batch(批量学习)
  - Instance-based(基于实例学习)
  - Model-based(基于模型学习)
  
#### 监督学习
监督学习，训练数据包含目标结论(目标结论也叫“标签”)。

![](pic/c1-01.png)

  - 监督学习的一个典型的任务是: 分类(Classification)，比如垃圾邮件过滤器，用很多垃圾邮件的案例来训练，然后学习如何分类新邮件。
  - 监督学习的另一个典型任务是: 回归(Regression)，预测(Predict)一个目标数值，比如二手车的价格，给出一组功能数据(里程数，车龄，品牌等)，这些数据叫做“预测器”(predictors)。
  训练系统时，需要提供很多二手车的案例数据，包括"预测器"和"标签(即价格)"。

![](pic/c1-02.png)

回归算法和分类算法可以互用，逻辑回归(Logistic Regression)一般用于分类，因为它可以输出一个对应某个给定分类的概率值(比如20%概率为垃圾邮件)。
监督学习的重要算法有:
  - K最近邻算法(k-Nearest Neighbors)
  - 线性回归(Linear Regression)
  - 逻辑回归(Logistic Regression)
  - 支持向量机(Support Vector Machines (SVMs))
  - 决策树和随机森林(Decision Trees and Random Forests)
  - 神经网络(Neural networks)，(神经网络也可以是非监督学习)
  
#### 非监督学习
非监督学习，训练数据不包含目标结论(目标结论也叫“标签”)，系统不需要被教导如何学习。

![](pic/c1-03.png)

非监督学习的重要算法有:
  - 聚类算法(Clustering)
    - k-Means 算法
    - 层次聚类算法(Hierarchical Cluster Analysis (HCA))
    - 最大期望算法(Expectation Maximization (EM))
  - 可视化和降维 (Visualization and dimensionality reduction)
    — 主成分分析(Principal Component Analysis (PCA))
    — 核心主成分分析(Kernel PCA)
    — 局部线性嵌入(Locally-Linear Embedding (LLE))
    — t-分布随机邻域嵌入算法(t-distributed Stochastic Neighbor Embedding (t-SNE))
  - 关联规则学习(Association rule learning)
    - Apriori 算法
    - Eclat 算法

- 比如有一些自己Blog的访客数据，聚类算法可以检测出相似的访客，算法在不提前知道访客属于哪个分类的情况下，可以找独立找出分类，
例如算法可以找出40%的访客是男性，并且在晚上访问自己的Blog，20%的访客是科幻小说迷，并且喜欢在周末访问自己的Blog。
使用层次聚类算法可以把这个分类拆分的更细。

![](pic/c1-04.png)

- 可视化算法也是个好例子：给出大量复杂的无标签的数据，算法输出2D或3D的可以被绘制数据表现。
算法保存尽可能多的结构(比如在重合的可视化图中保持不同的聚类)，这样就可以理解数据如何组织并识别未知的模式。

![](pic/c1-05.png)

- 一个降维的目标是：不丢失太多信息的前提下简化数据。
一种方式是，把几个想关的功能合并为一个，比如车的里程数与车龄非常相关，所以降维算法会把这些数据合并为一个数据：车的磨损。

- 另一个重要的任务是异常检测(Anomaly Detection)，
比如：检测信用卡的非正常交易以防止欺诈；捕捉生产缺陷，自动从数据集中移除异常值，以免进入另一个学习算法。
系统用正常数据训练，当检测新数据时可以判断是否正常。 

![](pic/c1-06.png)

- 关联规则学习：目标是从大量数据中挖掘出感兴趣的关系。
比如运营一个超市时，在销售日志中可以发现发烤肉酱和土豆片的人通常还买牛排，这样就可以把这些东西的货架放置在一起。

#### 半监督学习
半监督学习，处理只有部分数据打标签的情况，一般是大量未标记数据的和少量标记的数据。
比如使用Google的照片云存储上传照片时，算法能自动识别同一个人A在照片1，5，11中，另一个人B在照片2，5，7中，这就是聚类算法的非监督学习。
系统需要你告诉它这些人是谁，每个人设置一个标签，系统就可以在所有照片中找到所有的人了。

![](pic/c1-07.png)

- 大多数半监督学习算法是监督学习和非监督学习的组合。
比如，深度信念网络(deep belief networks (DBNs)) 基于非监督学习的一个组件，叫做“有限Boltzmann机(restricted Boltzmann machines (RBMs))”，
RBMs 以非监督学习的方式有序地训练，然后整系统使用监督学习技术调优。

#### 强化学习
强化学习大不一样。学习系统叫做 “Agent”，可以观察环境，选择并执行“Action”，得到返回的"Rewards"，或得到负面的“Penalties”。
学习系统必须自己学习什么是最好的策略，策略叫做“Policy”，以获取最好的"Rewards"，“Policy” 定义了"Agent"在所处的情况下应当选择的最好的“Action”。

![](pic/c1-08.png)

- 比如机器人实现了强化学习，以学习如何行走。AlphaGo 打败了李世石，AlphaGo 通过分析几百万的对战来学习战胜策略，并且与自己对战了很多次。
注意: 学习系统在对战李世石时是关闭了的。AlphaGo只是运用了已经学到的策略。

#### 离线学习(批量学习)
批量学习，系统不会渐近式学习，必须用所有已有的数据训练。这样会花大量时间和计算资源，所以一般是离线完成的。
一旦系统被训练了，然后就会进入产品阶段运行，不会再学习了。只是运用已经学会的东西，也叫“离线学习”。

- 如果想让学习系统学习新的数据，需要从0开始训练新版本的系统(新老数据一起训练)，然后停止老系统，用新系统替换。
- 根据数据更新的频度，需要制定适合的训练频率，比如每24小时训练一次，一次几小时
- 训练数据需要大量的CPU、IO、内存、磁盘资源，所以如果数据量很大，成本有限，环境有限，则是不可能完成的(比如在手机上)

#### 在线学习(渐近学习)
在线学习，有序地提供一条或一小组新数据，渐近式地学习，新数据叫做小批量数据(mini-batches)。
每个学习步骤都很便宜快捷，这样系统就可以在新数据到来时在线学习新数据。

![](pic/c1-09.png)

- 在线学习适合不间断地数据流(比如股票价格)，然后快速或自动地适配。
- 还有一种场景是计算资源有限，学习系统学习了新数据后，老数据可以被丢弃，以节省存储空间。
- 还适用于当数据集大到内存无法放下时，叫做out-of-core学习(核外学习)，系统加载部分数据，然后一步步计算。

![](pic/c1-10.png)

一个重要的数数是：系统能够以多快的速度适配新数据，叫做“学习率(learning rate)”。
- 如果学习率高，系统可以快速学习新数据，但也会快速忘记老数据。
- 如果学习率低，系统会比较迟钝，但也不会被新数据中的无效数据那么敏感。

对于在线学习的挑战是：坏数据被喂进系统，系统性能会下降，尤其在在线系统中，客户会察觉到，所以需要异常检测(Anomaly Detection)算法。

#### 基于实例学习
大多数机器学习的任务是：预测。也就是给出一定量的示例训练数据，系统需要能泛化没有见过的示例数据。
最琐碎的学习形式大概是用心学习。如果用这种方式创建垃圾邮件过滤器，应该会把之前标记过的邮件继续标记为垃圾邮件，听起来也不是太坏的方案。

另一种方案是：把与之前标记过的相似的垃圾邮件标记为垃圾邮件。
这需要比较相似度(measure of similarity)，最基本的相似度比较是查找共有的字符串数量。

这叫做“基于实例学习”，先用心学习示例数据，然后比较相似度来泛化(generalize)新数据。

![](pic/c1-11.png)

#### 基于模型学习
另一种泛化(generalize)的方式是构建一个数据模型，然后用模型去预测。叫做“基于模型学习”。

![](pic/c1-12.png)

举例，你想知道钱是否令人幸福，然后从OECD网站下载Better Life Index数据，从IMF下载GDP数据，然后联表查询。

![](pic/c1-13.png)

![](pic/c1-14.png)

看起来确实有这个趋势，虽然数据有点噪声，但基本上钱是令人幸福的。所以你可以把生活满意度构建成一个人均GDP的线性函数模型。
这个过程叫做**模型选择(model selection)**：选择了一个生活满意度的线性模型，只有一个因素：人均GDP。

```
lifeSatisfaction = θ<sup>0</sup> + θ<sup>1</sup> × GDP_per_capita
```
这个模型有两个参数：θ<sup>0</sup>，θ<sup>1</sup>，对参数做一些调整后，模型可以表示任何线性函数。如下图：

![](pic/c1-15.png)

在使用模型前，需要定义参数θ<sup>0</sup>，θ<sup>1</sup>的值。怎样才能知道什么样的值让模型表现最优？需要确定一个性能指标才能回答这个问题。可以定义一个工具函数(utility function)来衡量模型有多好，也可以定义一个价值函数(cost function)来衡量模型有多差。对于线性回归问题，人们一般用价值函数(cost function)来衡量线性模型的预测与训练数据的差距，目标就是减少差距。

这也是线性回归算法的由来，你提供训练数据，算法找到最佳的适配数据的线性模型。这过程叫做**训练模型**，在上面这个例子中，最优值是
```
θ<sup>0</sup>=4.85，θ<sup>1</sup>=4.91 × 10<sub>–5</sub>
```
现在你想预测塞浦路斯人的幸福程度，塞浦路斯的人均GDP是$22,587，然后满意度的值是 4.85 + 22,587 × 4.91 × 10<sub>–5</sub> = 5.96

![](pic/c1-16.png)

如果不满意这个模型的话，可能需要加入更多因素
，比如就业率，健康水平，空气质量等，取得更多或更优质的训练数据，或选择更强大的模型(比如多项式回归模型)。

总结开来，过程如下：
 1. 学习数据
 2. 选择模型
 3. 在训练数据上调优模型
 4. 应用模型到新数据上

这就是典型的机器学习项目的样子。

### 机器学习的挑战: 不好的数据
简单地说，因为主要的任务是选择一个学习算法然后在这之上训练数据，然而**不好的算法**和**不好的数据**可能会破坏这个系统。

#### 训练数据量不足
小孩学习识别苹果的时候，你拿到一个苹果，告诉它这是苹果，可能重复几次这个过程，小孩就能识别不同颜色、形状的苹果了。

机器学习没这么优秀，需要大量的数据让大多数机器学习算法运行起来。即使是很简单的问题也需要几千个训练数据，而复杂的问题就需要百万级的数据量了，除非可以用现有的模型。

#### 训练数据没有代表性
训练数据要有代表性，并且能代表新数据的情况，才能更好的泛化(generalize)，不管是基于实例还是基于模型的学习都一样。

比如之前的国家数据集，用于训练线性模型，就不够具有代表性，少数国家缺数据，下图是把这些国家的数据添加上的情况：

![](pic/c1-18.png)

你用这个数据训练，就能得到实线，而之前的训练，只能得到虚线。你会发现，添加一些国家的数据不光会明显地改变模型，这样简单的模型可能永远也不会正确。看起来似乎很富有的国家也没有比中等富有的国家更幸福，甚至穷国家看起来比富国家更幸福。因为使用了不具有的代表性的数据，模型不能精确的预测结果，尤其是很穷和很富的国家。

使用具有代表性的数据也没那么容易，如果取样量很小，就会有抽样噪声; 即使取样量很大，如果取样方法不正确，也可能不具有代表性，这叫做**抽样偏差(sampling bias)**。

#### 训练数据质量差
如果训练数据全都是错误、异常值、噪声，会使系统更难探测出下层的模式，所以值得花时间去清理数据。事实上，很多数据科学家花大量的时间做这件事情。比如：
  - 如果实例数据有异常值，可以直接丢弃或手动修复。
  - 如果实例数据缺少特征值(比如5%的用户不填年龄)，必须要决定是否忽略这个因素，还是忽略这些实例数据，还是填充这些数据(取年龄中值)。
  
#### 不相关的特征值
只有当训练数据足够相关，并且没有很多不相关的数据时，系统才可能有用。

机器学习的项目的成功关键是：训练好的数据集。这个过程叫做：**特征工程(feature engineering)**，包括：
  - 特征选择: 选取大量有用的特征，来训练已有的特征。
  - 特征筛选: 组合已有的特征，得出更有用的特征(比如 降维)。
  - 通过收集新数据，创建新特征。

#### 过度拟合训练数据
当你去一个国家游玩时被出租车宰客了，你可能会认为这个国家的所有出租车司机都是骗子。过度泛化(以偏概全, Overgeneralizing)是人类常干的。
如果不好好处理，机器可能也会这样。在机器学习中，这叫做：**过度拟合(overfitting)**。意思是说：模型训练数据训练的很好，但是没有很好的泛化。

下图是一个高度多项式生活满意度模型，过度拟合了训练数据。虽然比简单的线性模型要好一些，但还是不准确。

![](pic/c1-19.png)

复杂的模型，比如深度神经网络(deep neural networks)可以检测出数据中难以察觉的模式。但如何数据集中噪声太多，或数据集太小(引入了取样噪声)，那么模型就可能会探测到噪声本身。显然这些模式不会泛化新的实例数据。比如，你给生活满意度模型提供了很多因素，包括无国家名称的数据。在这种情况下，复杂的数据模型可能探测出含有字母w的国家的生活满意度大于7，比如 New Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5)，这个模型对于Rwanda 或 Zimbabwe 就不行了，显然这个模型纯粹是靠运气，但是模型自己没办法知道模型是正确的还是因为数据有噪声。

当模型相当于训练数据的噪声过于复杂时，会发生过度拟合，解决方法是：
  - 简化模型，用线性模型取代多项式模型
  - 获取更多训练数据
  - 减少训练数据的噪声

约束模型，以减少模型过度拟合的风险，叫做**正则化(regularization)**。
下图中有三个模型：
  - 点线是原模型，用缺少一些国家的数据训练的
  - 虚线模型是用所有国家的数据训练的
  - 实线的数据与第一种相同，但是经过正则约束。
  
正则化使得模型的斜率更小，这样使模型更少的适应已经训练了的数据，但是可以更好的泛化新的采样。

![](pic/c1-20.png)

可以用一个超级参数(hyperparameter)来控制正则化的总量。超级参数是学习算法的一个参数(而不是模型的参数)。超级参数调化是构建机器学习系统中一项重要的工作。

#### 低度拟合训练数据
低度拟合(overfitting)，与过度拟合相反，发生在模型太简单，以至于无法学习数据的下层结构。比如线性的生活满意度模型就有点低度拟合。
有这些解决办法：
  - 选择更有力的模型(带更多参数)
  - 给学习算法提供更好的特征
  - 在模型上减少约束(比如减少正则化超级参数)
